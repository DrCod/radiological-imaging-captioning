{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from base_model import BaseModel\n",
    "\n",
    "class CaptionGenerator(BaseModel):\n",
    "    def build(self):\n",
    "        \"\"\" Build the model. \"\"\"\n",
    "        self.build_cnn()\n",
    "        self.build_rnn()\n",
    "        if self.is_train:\n",
    "            self.build_optimizer()\n",
    "            self.build_summary()\n",
    "\n",
    "    def build_cnn(self):\n",
    "        \"\"\" Build the CNN. \"\"\"\n",
    "        print(\"Building the CNN...\")\n",
    "        if self.config.cnn == 'model':\n",
    "            self.build_model()\n",
    "        print(\"CNN built.\")\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\" Build the CNN net. \"\"\"\n",
    "        config = self.config\n",
    "\n",
    "        images = tf.placeholder(\n",
    "            dtype = tf.float32,\n",
    "            shape = [config.batch_size] + self.image_shape)\n",
    "\n",
    "        conv1_1_feats = self.nn.conv2d(images, 32, name = 'conv1_1')\n",
    "        conv1_2_feats = self.nn.conv2d(conv1_1_feats, 32, name = 'conv1_2')\n",
    "        pool1_feats = self.nn.max_pool2d(conv1_2_feats, name = 'pool1')\n",
    "\n",
    "        conv2_1_feats = self.nn.conv2d(pool1_feats, 64, name = 'conv2_1')\n",
    "        conv2_2_feats = self.nn.conv2d(conv2_1_feats, 64, name = 'conv2_2')\n",
    "        pool2_feats = self.nn.max_pool2d(conv2_2_feats, name = 'pool2')\n",
    "\n",
    "        conv5_1_feats = self.nn.conv2d(pool4_feats, 64, name = 'conv3_1')\n",
    "        conv5_2_feats = self.nn.conv2d(conv5_1_feats, 64, name = 'conv3_2')\n",
    "        conv5_3_feats = self.nn.conv2d(conv5_2_feats, 64, name = 'conv3_3')\n",
    "\n",
    "        reshaped_conv5_3_feats = tf.reshape(conv5_3_feats,\n",
    "                                            [config.batch_size, 28, 64])\n",
    "\n",
    "        self.conv_feats = reshaped_conv5_3_feats\n",
    "        self.num_ctx = 28\n",
    "        self.dim_ctx = 64\n",
    "        self.images = images\n",
    "\n",
    "    \n",
    "    def build_rnn(self):\n",
    "        \"\"\" Build the RNN. \"\"\"\n",
    "        print(\"Building the RNN...\")\n",
    "        config = self.config\n",
    "\n",
    "        # Setup the placeholders\n",
    "        if self.is_train:\n",
    "            contexts = self.conv_feats\n",
    "            sentences = tf.placeholder(\n",
    "                dtype = tf.int32,\n",
    "                shape = [config.batch_size, config.max_caption_length])\n",
    "            masks = tf.placeholder(\n",
    "                dtype = tf.float32,\n",
    "                shape = [config.batch_size, config.max_caption_length])\n",
    "        else:\n",
    "            contexts = tf.placeholder(\n",
    "                dtype = tf.float32,\n",
    "                shape = [config.batch_size, self.num_ctx, self.dim_ctx])\n",
    "            last_memory = tf.placeholder(\n",
    "                dtype = tf.float32,\n",
    "                shape = [config.batch_size, config.num_lstm_units])\n",
    "            last_output = tf.placeholder(\n",
    "                dtype = tf.float32,\n",
    "                shape = [config.batch_size, config.num_lstm_units])\n",
    "            last_word = tf.placeholder(\n",
    "                dtype = tf.int32,\n",
    "                shape = [config.batch_size])\n",
    "\n",
    "        # Setup the word embedding\n",
    "        with tf.variable_scope(\"word_embedding\"):\n",
    "            embedding_matrix = tf.get_variable(\n",
    "                name = 'weights',\n",
    "                shape = [config.vocabulary_size, config.dim_embedding],\n",
    "                initializer = self.nn.fc_kernel_initializer,\n",
    "                regularizer = self.nn.fc_kernel_regularizer,\n",
    "                trainable = self.is_train)\n",
    "\n",
    "        # Setup the LSTM\n",
    "        lstm = tf.nn.rnn_cell.LSTMCell(\n",
    "            config.num_lstm_units,\n",
    "            initializer = self.nn.fc_kernel_initializer)\n",
    "        if self.is_train:\n",
    "            lstm = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm,\n",
    "                input_keep_prob = 1.0-config.lstm_drop_rate,\n",
    "                output_keep_prob = 1.0-config.lstm_drop_rate,\n",
    "                state_keep_prob = 1.0-config.lstm_drop_rate)\n",
    "\n",
    "        # Initialize the LSTM using the mean context\n",
    "        with tf.variable_scope(\"initialize\"):\n",
    "            context_mean = tf.reduce_mean(self.conv_feats, axis = 1)\n",
    "            initial_memory, initial_output = self.initialize(context_mean)\n",
    "            initial_state = initial_memory, initial_output\n",
    "\n",
    "        # Prepare to run\n",
    "        predictions = []\n",
    "        if self.is_train:\n",
    "            alphas = []\n",
    "            cross_entropies = []\n",
    "            predictions_correct = []\n",
    "            num_steps = config.max_caption_length\n",
    "            last_output = initial_output\n",
    "            last_memory = initial_memory\n",
    "            last_word = tf.zeros([config.batch_size], tf.int32)\n",
    "        else:\n",
    "            num_steps = 1\n",
    "        last_state = last_memory, last_output\n",
    "\n",
    "        # Generate the words one by one\n",
    "        for idx in range(num_steps):\n",
    "            # Attention mechanism\n",
    "            with tf.variable_scope(\"attend\"):\n",
    "                alpha = self.attend(contexts, last_output)\n",
    "                context = tf.reduce_sum(contexts*tf.expand_dims(alpha, 2),\n",
    "                                        axis = 1)\n",
    "                if self.is_train:\n",
    "                    tiled_masks = tf.tile(tf.expand_dims(masks[:, idx], 1),\n",
    "                                         [1, self.num_ctx])\n",
    "                    masked_alpha = alpha * tiled_masks\n",
    "                    alphas.append(tf.reshape(masked_alpha, [-1]))\n",
    "\n",
    "            # Embed the last word\n",
    "            with tf.variable_scope(\"word_embedding\"):\n",
    "                word_embed = tf.nn.embedding_lookup(embedding_matrix,\n",
    "                                                    last_word)\n",
    "           # Apply the LSTM\n",
    "            with tf.variable_scope(\"lstm\"):\n",
    "                current_input = tf.concat([context, word_embed], 1)\n",
    "                output, state = lstm(current_input, last_state)\n",
    "                memory, _ = state\n",
    "\n",
    "            # Decode the expanded output of LSTM into a word\n",
    "            with tf.variable_scope(\"decode\"):\n",
    "                expanded_output = tf.concat([output,\n",
    "                                             context,\n",
    "                                             word_embed],\n",
    "                                             axis = 1)\n",
    "                logits = self.decode(expanded_output)\n",
    "                probs = tf.nn.softmax(logits)\n",
    "                prediction = tf.argmax(logits, 1)\n",
    "                predictions.append(prediction)\n",
    "\n",
    "            # Compute the loss for this step, if necessary\n",
    "            if self.is_train:\n",
    "                cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels = sentences[:, idx],\n",
    "                    logits = logits)\n",
    "                masked_cross_entropy = cross_entropy * masks[:, idx]\n",
    "                cross_entropies.append(masked_cross_entropy)\n",
    "\n",
    "                ground_truth = tf.cast(sentences[:, idx], tf.int64)\n",
    "                prediction_correct = tf.where(\n",
    "                    tf.equal(prediction, ground_truth),\n",
    "                    tf.cast(masks[:, idx], tf.float32),\n",
    "                    tf.cast(tf.zeros_like(prediction), tf.float32))\n",
    "                predictions_correct.append(prediction_correct)\n",
    "\n",
    "                last_output = output\n",
    "                last_memory = memory\n",
    "                last_state = state\n",
    "                last_word = sentences[:, idx]\n",
    "\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "        # Compute the final loss, if necessary\n",
    "        if self.is_train:\n",
    "            cross_entropies = tf.stack(cross_entropies, axis = 1)\n",
    "            cross_entropy_loss = tf.reduce_sum(cross_entropies) \\\n",
    "                                 / tf.reduce_sum(masks)\n",
    "\n",
    "            alphas = tf.stack(alphas, axis = 1)\n",
    "            alphas = tf.reshape(alphas, [config.batch_size, self.num_ctx, -1])\n",
    "            attentions = tf.reduce_sum(alphas, axis = 2)\n",
    "            diffs = tf.ones_like(attentions) - attentions\n",
    "            attention_loss = config.attention_loss_factor \\\n",
    "                             * tf.nn.l2_loss(diffs) \\\n",
    "                             / (config.batch_size * self.num_ctx)\n",
    "\n",
    "            reg_loss = tf.losses.get_regularization_loss()\n",
    "\n",
    "            total_loss = cross_entropy_loss + attention_loss + reg_loss\n",
    "\n",
    "            predictions_correct = tf.stack(predictions_correct, axis = 1)\n",
    "            accuracy = tf.reduce_sum(predictions_correct) \\\n",
    "                       / tf.reduce_sum(masks)\n",
    "\n",
    "        self.contexts = contexts\n",
    "        if self.is_train:\n",
    "            self.sentences = sentences\n",
    "            self.masks = masks\n",
    "            self.total_loss = total_loss\n",
    "            self.cross_entropy_loss = cross_entropy_loss\n",
    "            self.attention_loss = attention_loss\n",
    "            self.reg_loss = reg_loss\n",
    "            self.accuracy = accuracy\n",
    "            self.attentions = attentions\n",
    "        else:\n",
    "            self.initial_memory = initial_memory\n",
    "            self.initial_output = initial_output\n",
    "            self.last_memory = last_memory\n",
    "            self.last_output = last_output\n",
    "            self.last_word = last_word\n",
    "            self.memory = memory\n",
    "            self.output = output\n",
    "            self.probs = probs\n",
    "\n",
    "        print(\"RNN built.\")\n",
    "\n",
    "    def initialize(self, context_mean):\n",
    "        \"\"\" Initialize the LSTM using the mean context. \"\"\"\n",
    "        config = self.config\n",
    "        context_mean = self.nn.dropout(context_mean)\n",
    "        if config.num_initalize_layers == 1:\n",
    "            # use 1 fc layer to initialize\n",
    "            memory = self.nn.dense(context_mean,\n",
    "                                   units = config.num_lstm_units,\n",
    "                                   activation = None,\n",
    "                                   name = 'fc_a')\n",
    "            output = self.nn.dense(context_mean,\n",
    "                                   units = config.num_lstm_units,\n",
    "                                   activation = None,\n",
    "                                   name = 'fc_b')\n",
    "        else:\n",
    "            # use 2 fc layers to initialize\n",
    "            temp1 = self.nn.dense(context_mean,\n",
    "                                  units = config.dim_initalize_layer,\n",
    "                                  activation = tf.tanh,\n",
    "                                  name = 'fc_a1')\n",
    "            temp1 = self.nn.dropout(temp1)\n",
    "            memory = self.nn.dense(temp1,\n",
    "                                   units = config.num_lstm_units,\n",
    "                                   activation = None,\n",
    "                                   name = 'fc_a2')\n",
    "\n",
    "            temp2 = self.nn.dense(context_mean,\n",
    "                                  units = config.dim_initalize_layer,\n",
    "                                  activation = tf.tanh,\n",
    "                                  name = 'fc_b1')\n",
    "            temp2 = self.nn.dropout(temp2)\n",
    "            output = self.nn.dense(temp2,\n",
    "                                   units = config.num_lstm_units,\n",
    "                                   activation = None,\n",
    "                                   name = 'fc_b2')\n",
    "        return memory, output\n",
    "\n",
    "    def attend(self, contexts, output):\n",
    "        \"\"\" Attention Mechanism. \"\"\"\n",
    "        config = self.config\n",
    "        reshaped_contexts = tf.reshape(contexts, [-1, self.dim_ctx])\n",
    "        reshaped_contexts = self.nn.dropout(reshaped_contexts)\n",
    "        output = self.nn.dropout(output)\n",
    "        if config.num_attend_layers == 1:\n",
    "            # use 1 fc layer to attend\n",
    "            logits1 = self.nn.dense(reshaped_contexts,\n",
    "                                    units = 1,\n",
    "                                    activation = None,\n",
    "                                    use_bias = False,\n",
    "                                    name = 'fc_a')\n",
    "            logits1 = tf.reshape(logits1, [-1, self.num_ctx])\n",
    "            logits2 = self.nn.dense(output,\n",
    "                                    units = self.num_ctx,\n",
    "                                    activation = None,\n",
    "                                    use_bias = False,\n",
    "                                    name = 'fc_b')\n",
    "            logits = logits1 + logits2\n",
    "        else:\n",
    "            # use 2 fc layers to attend\n",
    "            temp1 = self.nn.dense(reshaped_contexts,\n",
    "                                  units = config.dim_attend_layer,\n",
    "                                  activation = tf.tanh,\n",
    "                                  name = 'fc_1a')\n",
    "            temp2 = self.nn.dense(output,\n",
    "                                  units = config.dim_attend_layer,\n",
    "                                  activation = tf.tanh,\n",
    "                                  name = 'fc_1b')\n",
    "            temp2 = tf.tile(tf.expand_dims(temp2, 1), [1, self.num_ctx, 1])\n",
    "            temp2 = tf.reshape(temp2, [-1, config.dim_attend_layer])\n",
    "            temp = temp1 + temp2\n",
    "            temp = self.nn.dropout(temp)\n",
    "            logits = self.nn.dense(temp,\n",
    "                                   units = 1,\n",
    "                                   activation = None,\n",
    "                                   use_bias = False,\n",
    "                                   name = 'fc_2')\n",
    "            logits = tf.reshape(logits, [-1, self.num_ctx])\n",
    "        alpha = tf.nn.softmax(logits)\n",
    "        return alpha\n",
    "\n",
    "    def decode(self, expanded_output):\n",
    "        \"\"\" Decode the expanded output of the LSTM into a word. \"\"\"\n",
    "        config = self.config\n",
    "        expanded_output = self.nn.dropout(expanded_output)\n",
    "        if config.num_decode_layers == 1:\n",
    "            # use 1 fc layer to decode\n",
    "            logits = self.nn.dense(expanded_output,\n",
    "                                   units = config.vocabulary_size,\n",
    "                                   activation = None,\n",
    "                                   name = 'fc')\n",
    "        else:\n",
    "            # use 2 fc layers to decode\n",
    "            temp = self.nn.dense(expanded_output,\n",
    "                                 units = config.dim_decode_layer,\n",
    "                                 activation = tf.tanh,\n",
    "                                 name = 'fc_1')\n",
    "            temp = self.nn.dropout(temp)\n",
    "            logits = self.nn.dense(temp,\n",
    "                                   units = config.vocabulary_size,\n",
    "                                   activation = None,\n",
    "                                   name = 'fc_2')\n",
    "        return logits\n",
    "\n",
    "    def build_optimizer(self):\n",
    "        \"\"\" Setup the optimizer and training operation. \"\"\"\n",
    "        config = self.config\n",
    "\n",
    "        learning_rate = tf.constant(config.initial_learning_rate)\n",
    "        if config.learning_rate_decay_factor < 1.0:\n",
    "            def _learning_rate_decay_fn(learning_rate, global_step):\n",
    "                return tf.train.exponential_decay(\n",
    "                    learning_rate,\n",
    "                    global_step,\n",
    "                    decay_steps = config.num_steps_per_decay,\n",
    "                    decay_rate = config.learning_rate_decay_factor,\n",
    "                    staircase = True)\n",
    "            learning_rate_decay_fn = _learning_rate_decay_fn\n",
    "        else:\n",
    "            learning_rate_decay_fn = None\n",
    "\n",
    "        with tf.variable_scope('optimizer', reuse = tf.AUTO_REUSE):\n",
    "            if config.optimizer == 'Adam':\n",
    "                optimizer = tf.train.AdamOptimizer(\n",
    "                    learning_rate = config.initial_learning_rate,\n",
    "                    beta1 = config.beta1,\n",
    "                    beta2 = config.beta2,\n",
    "                    epsilon = config.epsilon\n",
    "                    )\n",
    "            elif config.optimizer == 'RMSProp':\n",
    "                optimizer = tf.train.RMSPropOptimizer(\n",
    "                    learning_rate = config.initial_learning_rate,\n",
    "                    decay = config.decay,\n",
    "                    momentum = config.momentum,\n",
    "                    centered = config.centered,\n",
    "                    epsilon = config.epsilon\n",
    "                )\n",
    "            elif config.optimizer == 'Momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(\n",
    "                    learning_rate = config.initial_learning_rate,\n",
    "                    momentum = config.momentum,\n",
    "                    use_nesterov = config.use_nesterov\n",
    "                )\n",
    "            else:\n",
    "                optimizer = tf.train.GradientDescentOptimizer(\n",
    "                    learning_rate = config.initial_learning_rate\n",
    "                )\n",
    "\n",
    "            opt_op = tf.contrib.layers.optimize_loss(\n",
    "                loss = self.total_loss,\n",
    "                global_step = self.global_step,\n",
    "                learning_rate = learning_rate,\n",
    "                optimizer = optimizer,\n",
    "                clip_gradients = config.clip_gradients,\n",
    "                learning_rate_decay_fn = learning_rate_decay_fn)\n",
    "\n",
    "        self.opt_op = opt_op\n",
    "\n",
    "    def build_summary(self):\n",
    "        \"\"\" Build the summary (for TensorBoard visualization). \"\"\"\n",
    "        with tf.name_scope(\"variables\"):\n",
    "            for var in tf.trainable_variables():\n",
    "                with tf.name_scope(var.name[:var.name.find(\":\")]):\n",
    "                    self.variable_summary(var)\n",
    "\n",
    "        with tf.name_scope(\"metrics\"):\n",
    "            tf.summary.scalar(\"cross_entropy_loss\", self.cross_entropy_loss)\n",
    "            tf.summary.scalar(\"attention_loss\", self.attention_loss)\n",
    "            tf.summary.scalar(\"reg_loss\", self.reg_loss)\n",
    "            tf.summary.scalar(\"total_loss\", self.total_loss)\n",
    "            tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "\n",
    "        with tf.name_scope(\"attentions\"):\n",
    "            self.variable_summary(self.attentions)\n",
    "\n",
    "        self.summary = tf.summary.merge_all()\n",
    "\n",
    "    def variable_summary(self, var):\n",
    "        \"\"\" Build the summary for a variable. \"\"\"\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
